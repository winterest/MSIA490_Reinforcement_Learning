# -*- coding: utf-8 -*-
"""hw2-pong-v0.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/11zJtHrA6aL9_gBM4kgYSYcsfLE7UHzMU
"""

# Commented out IPython magic to ensure Python compatibility.
import gym
import numpy as np
import matplotlib.pyplot as plt
from tqdm import tqdm, trange
import pandas as pd
import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
from torch.autograd import Variable
from torch.distributions import Categorical
# %matplotlib inline

GAMMA = 0.99
EPISODE = 500

env = gym.make('Pong-v0')
env.seed(1)

def preprocess(image):
    """ prepro 210x160x3 uint8 frame into 6400 (80x80) 2D float array """
    image = image[35:195] # crop
    image = image[::2,::2,0] # downsample by factor of 2
    image[image == 144] = 0 # erase background (background type 1)
    image[image == 109] = 0 # erase background (background type 2)
    image[image != 0] = 1 # everything else (paddles, ball) just set to 1
    return np.reshape(image.astype(np.float).ravel(), [6400])

env.action_space.n

class Policy(nn.Module):
    def __init__(self, hidden_size=64, dropout=0.5):
        super(Policy, self).__init__()
        self.state_space = 6400
        self.action_space = 2
        
        self.l1 = nn.Linear(self.state_space, hidden_size, bias=False)
        self.l2 = nn.Linear(hidden_size, self.action_space, bias=False)
        
        self.gamma = GAMMA
        self.dropout = dropout
        
        # Episode policy and reward history 
        self.policy_history = Variable(torch.Tensor()) 
        self.reward_episode = []
        # Overall reward and loss history
        self.reward_history = []
        self.loss_history = []

    def forward(self, x):
        model = torch.nn.Sequential(
            self.l1,
            nn.Dropout(p=self.dropout),
            nn.ReLU(),
            self.l2,
            nn.Softmax(dim=-1)
        )
        return model(x)

def select_action(state):
    #Select an action (0 or 1) by running policy model and choosing based on the probabilities in state
    state = torch.from_numpy(state).type(torch.FloatTensor).cuda()
    state = policy(Variable(state))
    c = Categorical(state)
    action = c.sample()

    
    # Add log probability of our chosen action to our history    
    if policy.policy_history.nelement() != 0:
        #print(policy.policy_history)
        #print(c.log_prob(action))
        policy.policy_history = torch.cat((policy.policy_history.view(-1), c.log_prob(action).view(1)),0)
    else:
        policy.policy_history = (c.log_prob(action))
    return action+2

policy = Policy(hidden_size=128,dropout=0.5).cuda()
optimizer = optim.Adam(policy.parameters(), lr=0.01)

for episode in range(EPISODE):
    state = env.reset() # Reset environment and record the starting state
    done = False       

    while(not done):
        state = preprocess(state)
        action = select_action(state)
        # Step through environment using chosen action
        #print(action.data)
        state, reward, done, _ = env.step( action.cpu().data.numpy())

        # Save reward
        policy.reward_episode.append(reward)

    R = 0
    rewards = []
    
    # Discount future rewards back to the present using gamma
    for r in policy.reward_episode[::-1]:
        R = r + policy.gamma * R
        rewards.insert(0,R)
        
    # Scale rewards
    rewards = torch.FloatTensor(rewards).cuda()
    rewards = (rewards - rewards.mean()) / (rewards.std() + np.finfo(np.float32).eps)
    
    # Calculate loss
    loss = (torch.sum(torch.mul(policy.policy_history, Variable(rewards)).mul(-1), -1))
    
    # Update network weights
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()
    
    #Save and intialize episode history counters
    policy.loss_history.append(loss.item())
    policy.reward_history.append(np.sum(policy.reward_episode))
    print(np.sum(policy.reward_episode))
    policy.policy_history = Variable(torch.Tensor())
    policy.reward_episode= []

policy.reward_history

